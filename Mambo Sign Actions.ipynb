{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For following a second Drone\n",
    "#Must have second drone in sights of Mambo Drone\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import zipfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_file = \"./traffic-signs-data/train.p\"\n",
    "validation_file= \"./traffic-signs-data/valid.p\"\n",
    "testing_file = \"./traffic-signs-data/valid.p\"\n",
    "\n",
    "with open(training_file, mode = 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(testing_file, mode = 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "with open(validation_file, mode = 'rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "    \n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "X_valid, y_valid = valid['features'], valid['features']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples =  34799\n",
      "Number of testing examples =  4410\n",
      "Image shape =  (32, 32, 3)\n",
      "Number of classes =  43\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "n_valid = len(X_valid)\n",
    "\n",
    "#Image shape and dimensions\n",
    "image_shape = X_train[0].shape\n",
    "#Number of classes and labels\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples = \", n_train)\n",
    "print(\"Number of testing examples = \", n_test)\n",
    "print(\"Image shape = \", image_shape)\n",
    "print(\"Number of classes = \", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_gray (34799, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "#Preprocess the Dataset \n",
    "\n",
    "#Grayscale \n",
    "\n",
    "X_train_gray = np.sum(X_train / 3, axis = 3, keepdims=True)\n",
    "X_valid_gray = np.sum(X_valid / 3, axis = 3, keepdims=True)\n",
    "X_test_gray = np.sum(X_test / 3, axis = 3, keepdims= True)\n",
    "print(\"X_train_gray\", X_train_gray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.354081335648\n"
     ]
    }
   ],
   "source": [
    "# Normalize images \n",
    "\n",
    "X_train_normalize = ((X_train_gray - 128) / 128)\n",
    "X_test_normalize = ((X_test_gray - 128) / 128)\n",
    "X_valid_normalize = ((X_valid_gray - 128) / 128)\n",
    "\n",
    "print(np.mean(X_train_normalize))\n",
    "X_valid = X_valid_normalize\n",
    "X_train = X_train_normalize\n",
    "X_test = X_test_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image Augmentation Functions \n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sharpen_image(img):\n",
    "    gb = cv2.GaussianBlur(img, (5,5), 20.0)\n",
    "    return cv2.addWeighted(img, 2, gb, -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_random(img):\n",
    "    np.random.seed(43)\n",
    "    rand_x = np.random.randint(-5, 5)\n",
    "    rand_y = np.random.randint(-5, 5)\n",
    "    translation_matrix = np.float32([ [1,0,rand_x], [0,1,rand_y]])\n",
    "    return cv2.warpAffine(img, translation_matrix, (32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(img):\n",
    "    noisy_image = skimage.util.random_noise(img, mode='gaussian', seed=None, clip=True) \n",
    "    return np.asarray(noisy_image, dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_all(data):\n",
    "    for x in data:\n",
    "        rand_number = random.randint(0, 6)\n",
    "        if rand_number == 0 or rand_number == 1:\n",
    "            x = add_noise(x)\n",
    "        if rand_number == 2:\n",
    "            x = translate_random(x)\n",
    "        if rand_number == 3:\n",
    "            x = sharpen_image(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 12\n",
    "BATCH_SIZE = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def lanet(x):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    strides = [1, 1, 1, 1]\n",
    "    strides_for_pool = [1, 2, 2, 1]\n",
    "    \n",
    "    #Input: 32, 32, 1 Output: 30, 30, 10\n",
    "    weights_0 = tf.Variable(tf.truncated_normal(shape = (3, 3, 1, 10), mean=mu, stddev=sigma))\n",
    "    bias_0 = tf.Variable(tf.zeros(10))\n",
    "    \n",
    "    conv_0 = tf.nn.conv2d(x, weights_0, strides, padding=\"VALID\") + bias_0\n",
    "    \n",
    "    #Activation\n",
    "    conv_0 = tf.nn.relu(conv_0)\n",
    "    conv_0 = tf.nn.dropout(conv_0, 0.9)\n",
    "    \n",
    "    #Input 30, 30, 10 Output: 28, 28, 100\n",
    "    weights_1 = tf.Variable(tf.truncated_normal(shape = (3, 3, 10, 100), mean=mu, stddev=sigma))\n",
    "    bias_1 = tf.Variable(tf.zeros(100))\n",
    "    \n",
    "    conv_1 = tf.nn.conv2d(conv_0, weights_1, strides, padding=\"VALID\") + bias_1\n",
    "    \n",
    "    #Activation\n",
    "    conv_0 = tf.nn.relu(conv_1)\n",
    "    \n",
    "    #Input 28, 28, 100 Output: 14, 14, 100\n",
    "    filter_shape = [1, 2, 2, 1]\n",
    "    pool_1 = tf.nn.max_pool(conv_1, filter_shape, strides_for_pool, padding= \"VALID\")\n",
    "    \n",
    "    #Input 14, 14, 100 Output 10, 10, 150\n",
    "    weights_2 = tf.Variable(tf.truncated_normal(shape=(5, 5, 100, 150), mean = mu, stddev=sigma))\n",
    "    bias_2 = tf.Variable(tf.zeros(150))\n",
    "    \n",
    "    conv_2 = tf.nn.conv2d(pool_1, weights_2, strides, padding= \"VALID\") + bias_2\n",
    "    \n",
    "    #Activation\n",
    "    conv_0 = tf.nn.relu(conv_0)\n",
    "    conv_0 = tf.nn.dropout(conv_0, 0.8)\n",
    "    \n",
    "    #Maxpool 10 10 150 output 5 5 150\n",
    "    filter_shape_2 = [1, 2, 2, 1]\n",
    "    pool_2 = tf.nn.max_pool(conv_2, filter_shape_2, strides_for_pool, padding=\"VALID\")\n",
    "    \n",
    "    #Input 5, 5, 150 Output 4, 4, 250\n",
    "    weights_3 = tf.Variable(tf.truncated_normal(shape=(2, 2, 150, 250), mean= mu, stddev=sigma))\n",
    "    bias_3 = tf.Variable(tf.zeros(250))\n",
    "    \n",
    "    conv_3 = tf.nn.conv2d(pool_2, weights_3, strides, padding= 'VALID') + bias_3\n",
    "    \n",
    "    #Activation \n",
    "    conv_3 = tf.nn.relu(conv_3)\n",
    "    conv_3 = tf.nn.dropout(conv_3, 0.7)\n",
    "    \n",
    "    #Maxpool 4, 4, 250 Output 2, 2, 250\n",
    "    filter_shape_3 = [1, 2, 2, 1]\n",
    "    pool_3 = tf.nn.max_pool(conv_3, filter_shape_3, strides_for_pool, padding='VALID')\n",
    "    \n",
    "    #Layer 4 Input 2 2 250 output 1 1 1000\n",
    "    weights_4 = tf.Variable(tf.truncated_normal(shape= (2, 2, 250, 1000), mean = mu, stddev = sigma))\n",
    "    bias_4 = tf.Variable(tf.zeros(1000))\n",
    "    \n",
    "    conv_4 = tf.nn.conv2d(pool_3, weights_4, strides, padding= \"VALID\") + bias_4\n",
    "    \n",
    "    #Activation\n",
    "    conv_4 = tf.nn.relu(conv_4)\n",
    "    conv_4 = tf.nn.dropout(conv_4, 0.6)\n",
    "    \n",
    "    \n",
    "\n",
    "    #Flatten\n",
    "    fc1 = tf.contrib.layers.flatten(conv_4)\n",
    "    \n",
    "    #Output 1000 to 500\n",
    "    wd2 = tf.Variable(tf.truncated_normal((1000, 500), mean=mu, stddev=sigma))\n",
    "    bd2 = tf.Variable(tf.zeros(500))\n",
    "    \n",
    "    fc2 = tf.add(tf.matmul(fc1, wd2), bd2)\n",
    "    \n",
    "    #Activate \n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    \n",
    "    #fc2 = tf.nn.dropout(fc2, dropout)\n",
    "    \n",
    "    #Flatten 500 to 300\n",
    "    \n",
    "    wd3 = tf.Variable(tf.truncated_normal((500, 300), mean=mu, stddev=sigma))\n",
    "    bd3 = tf.Variable(tf.zeros(300))\n",
    "    \n",
    "    fc3 = tf.add(tf.matmul(fc2, wd3), bd3)\n",
    "    \n",
    "    #Activate \n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    fc3 = tf.nn.dropout(fc3, 0.6)\n",
    "    \n",
    "    #Input 300 to 120\n",
    "    \n",
    "    wd4 = tf.Variable(tf.truncated_normal((300, 120), mean=mu, stddev= sigma))\n",
    "    bd4 = tf.Variable(tf.zeros(120))\n",
    "    \n",
    "    fc4 = tf.add(tf.matmul(fc3, wd4), bd4)\n",
    "    \n",
    "    #Activate\n",
    "    fc4 = tf.nn.relu(fc4)\n",
    "    #fc4 = tf.nn.dropout(fc4, dropout)\n",
    "    \n",
    "    #Input 120 to 84\n",
    "    \n",
    "    wd5 = tf.Variable(tf.truncated_normal((120, 84), mean=mu, stddev = sigma))\n",
    "    bd5 = tf.Variable(tf.zeros(84))\n",
    "    \n",
    "    fc5 = tf.add(tf.matmul(fc4, wd5), bd5)\n",
    "    \n",
    "    #Activate\n",
    "    fc5 = tf.nn.relu(fc5)\n",
    "    \n",
    "    #Flatten 84 to n_classes\n",
    "    \n",
    "    wd6 = tf.Variable(tf.truncated_normal((84, n_classes), mean=mu, stddev=sigma))\n",
    "    bd6 = tf.Variable(tf.zeros(n_classes))\n",
    "    \n",
    "    logits = tf.add(tf.matmul(fc5, wd6), bd6)\n",
    "    \n",
    "    #Activate \n",
    "    logits = tf.nn.relu(logits)\n",
    "    #logits = tf.nn.dropout(logits, dropout)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,(None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32,(None))\n",
    "#Giving values of either 1 or 0\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training Pipeline \n",
    "learning_rate = 0.003\n",
    "logits = lanet(x)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "training_operation = optimizer.minimize(loss_operation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model Visualization\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for i in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[i: i + BATCH_SIZE], y_data[i: i + BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict = {x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Running the Training model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    print(\"Train\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        #Augment Functions for shuffled Images\n",
    "        X_train = augment_all(X_train)\n",
    "        total_accuracy = 0\n",
    "        sess = tf.get_default_session()\n",
    "        for j in range(0, num_examples, BATCH_SIZE):\n",
    "            #Trained in Batch Sizes\n",
    "            batch_x, batch_y = X_train[j : j + BATCH_SIZE], y_train[j: j + BATCH_SIZE]\n",
    "            sess.run(training_operation, feed_dict = {x: batch_x, y: batch_y})\n",
    "        \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        \n",
    "        \n",
    "        print(\"EPOCHS\", (i + 1))\n",
    "        print(\"Validation Accuracy ... \", validation_accuracy)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        saver.save(sess, './boom1')\n",
    "    print(\"Model saved\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
